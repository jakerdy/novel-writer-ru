```powershell
#!/usr/bin/env pwsh
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ "—á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç—å" (PowerShell)

param(
  [Parameter(Mandatory=$true)][string]$File
)

Set-StrictMode -Version Latest
$ErrorActionPreference = 'Stop'

. "$PSScriptRoot/common.ps1"

$root = Get-ProjectRoot
$cfgProject = Join-Path $root "spec/knowledge/audit-config.json"
$cfgTemplate = Join-Path $root ".specify/templates/knowledge/audit-config.json"
$cfg = if (Test-Path $cfgProject) { $cfgProject } elseif (Test-Path $cfgTemplate) { $cfgTemplate } else { '' }

if (-not (Test-Path $File)) { throw "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: text-audit.ps1 -File <–ø—É—Ç—å>" }

python3 - << PY
import json, re, sys, os, math
path = r'''$File'''
cfg_path = r'''$cfg'''
text = open(path, 'r', encoding='utf-8', errors='ignore').read()
default_cfg = {
  "connector_phrases": ["È¶ñÂÖà","ÂÖ∂Ê¨°","ÂÜçÊ¨°","ÁÑ∂Âêé","ÁÑ∂ËÄå","ÊÄªËÄåË®Ä‰πã","Áªº‰∏äÊâÄËø∞","Âú®ÊüêÁßçÁ®ãÂ∫¶","‰ºóÊâÄÂë®Áü•","Âú®ÂΩì‰∏ã","ÈöèÁùÄ"],
  "empty_phrases": ["ÂπøÊ≥õÂÖ≥Ê≥®","ÂºïÂèëÁÉ≠ËÆÆ","ÂΩ±ÂìçÊ∑±Ëøú","ÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâ","ÊúâÊïàÊèêÂçá","ÂÖ∑Êúâ‰∏ÄÂÆöÁöÑÊåáÂØºÊÑè‰πâ","ÂÄºÂæóÊàë‰ª¨ÊÄùËÄÉ"],
  "cliche_pairs": [],
  "sentence_length": {"max_run_long":4, "max_run_short":5, "short_threshold":12, "long_threshold":35},
  "abstract_nouns": ["‰ª∑ÂÄº","ÊÑè‰πâ","ËÆ§Áü•","‰ΩìÁ≥ª","Ê®°Âºè","Ë∑ØÂæÑ","ÊñπÊ≥ïËÆ∫","Ë∂ãÂäø"],
  "min_concrete_details": 3
}
cfg = default_cfg
if cfg_path and os.path.exists(cfg_path):
  try: cfg.update(json.load(open(cfg_path,'r',encoding='utf-8')))
  except: pass
def count_occurrences(text, phrases):
  return {p: len(re.findall(re.escape(p), text)) for p in phrases if p}
def split_sentences(t):
  parts = re.split(r'[„ÄÇÔºÅÔºü!?\n]+', t)
  return [s.strip() for s in parts if s.strip()]
def sentence_lengths(sents):
  lens = [len(s) for s in sents]
  if not lens: return lens, 0, 0
  avg = sum(lens)/len(lens)
  var = sum((x-avg)**2 for x in lens)/len(lens)
  return lens, avg, var**0.5
def runs(lens, short_th, long_th):
  rs=rl=0; mrs=mrl=0
  for L in lens:
    if L<=short_th: rs+=1; mrs=max(mrs,rs); rl=0
    elif L>=long_th: rl+=1; mrl=max(mrl,rl); rs=0
    else: rs=rl=0
  return mrs, mrl
def abstract_density(sent, words):
  return sum(len(re.findall(re.escape(w), sent)) for w in words)
connectors = count_occurrences(text, cfg["connector_phrases"])
empties = count_occurrences(text, cfg["empty_phrases"])
sents = split_sentences(text)
lens, avg, std = sentence_lengths(sents)
mx_run_short, mx_run_long = runs(lens, cfg["sentence_length"]["short_threshold"], cfg["sentence_length"]["long_threshold"])
abstract_scores = [(i, abstract_density(s, cfg["abstract_nouns"])) for i,s in enumerate(sents)]
abstract_scores.sort(key=lambda x: x[1], reverse=True)
abstract_top = [(i,sents[i]) for i,sc in abstract_scores[:5] if sc>=2]
total_chars = len(text)
def ratio(c): return (c/max(1,total_chars))*1000
print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
print("üìä –û—Ç—á—ë—Ç –æ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ ¬´—á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç—å¬ª")
print(f"–§–∞–π–ª: {os.path.basename(path)}  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars}")
print("")
print("–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–≤—è–∑—É—é—â–∏—Ö —Å–ª–æ–≤ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞ —Ç—ã—Å—è—á—É —Å–∏–º–≤–æ–ª–æ–≤)")
tc=sum(connectors.values()); print(f"  –í—Å–µ–≥–æ: {tc}  | –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {ratio(tc):.2f}")
for k,v in sorted(connectors.items(), key=lambda x: -x[1])[:10]:
  if v>0: print(f"  - {k}: {v}")
print("")
print("–ü–æ–¥—Å—á—ë—Ç –ø—É—Å—Ç—ã—Ö/—à–∞–±–ª–æ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑")
te=sum(empties.values()); print(f"  –í—Å–µ–≥–æ: {te}  | –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {ratio(te):.2f}")
for k,v in sorted(empties.items(), key=lambda x: -x[1])[:10]:
  if v>0: print(f"  - {k}: {v}")
print("")
print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
print(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(lens)}  | –°—Ä–µ–¥–Ω–µ–µ: {avg:.1f}  | –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std:.1f}")
print(f"  –ú–∞–∫—Å. –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {mx_run_short} (–ø–æ—Ä–æ–≥ {cfg['sentence_length']['max_run_short']})")
print(f"  –ú–∞–∫—Å. –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {mx_run_long} (–ø–æ—Ä–æ–≥ {cfg['sentence_length']['max_run_long']})")
print("")
print("–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–∞—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∞ (–ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, ‚â•2 –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö —Å–ª–æ–≤–∞)")
if abstract_top:
  for idx,s in abstract_top:
    sn = s[:80] + ("‚Ä¶" if len(s)>80 else "")
    print(f"  - –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ {idx+1}: {sn}")
else:
  print("  –ù–µ—Ç —è–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–π –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏")
print("")
print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
print("  - –ó–∞–º–µ–Ω—è–π—Ç–µ –ø—É—Å—Ç—ã–µ —Ñ—Ä–∞–∑—ã –∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏/–ø—Ä–µ–¥–º–µ—Ç–∞–º–∏/–∑–∞–ø–∞—Ö–∞–º–∏")
print("  - –†–∞–∑–±–∏–≤–∞–π—Ç–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è; –æ–±—ä–µ–¥–∏–Ω—è–π—Ç–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∏—Ç–º–∞")
print("  - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –º–æ–∂–Ω–æ –ª–∏ —É–¥–∞–ª–∏—Ç—å —Å–≤—è–∑—É—é—â–∏–µ —Å–ª–æ–≤–∞ –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç—å –∏—Ö –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏")
print("  - –ü–µ—Ä–µ–¥ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ–º —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ 3 –¥–µ—Ç–∞–ª–∏ –∏–∑ –∂–∏–∑–Ω–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ—á–µ–∫")
PY
```